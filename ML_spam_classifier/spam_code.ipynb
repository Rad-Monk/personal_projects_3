{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find out the encoding of the dataset as it was giving an error while reading with pd\n",
    "# cleaning\n",
    "import chardet\n",
    "\n",
    "with open('E:/projects_I_will_never_do/pp_repo/personal_projects/ML_Email_spam_classifier/dataset/spam.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "df = pd.read_csv('E:/projects_I_will_never_do/pp_repo/personal_projects/ML_Email_spam_classifier/dataset/spam.csv', encoding=result['encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.rename(columns={'v1':'target','v2':'text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder= LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']= encoder.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#always check for missing and duplicates\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploratory data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "plt.pie(df['target'].value_counts(), labels=['ham','spam'],autopct=\"%0.2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_characters']=df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words']=df['text'].apply(lambda x: len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_sentences']=df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['target']== 0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['target']== 1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[df['target']== 0]['num_characters'])\n",
    "sns.histplot(df[df['target']== 1]['num_characters'],color='red')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# # Download NLTK data manually\n",
    "# nltk.download('stopwords', download_dir='E:/projects_I_will_never_do/pp_repo/personal_projects/ML_Email_spam_classifier')\n",
    "\n",
    "# # Set NLTK data path\n",
    "# nltk.data.path.append('E:/projects_I_will_never_do/pp_repo/personal_projects/ML_Email_spam_classifier')\n",
    "\n",
    "# # Now try to access stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps= PorterStemmer()\n",
    "ps.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "# lowercase>tokenise>remove special chars> sstop words> stemmin\n",
    "def transform_text(text):\n",
    "    text= text.lower()\n",
    "    text= nltk.word_tokenize(text)\n",
    "    y=[]\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "\n",
    "    return \" \".join(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_text(' Hi how Are You RAD @@-@@ :))))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.porter import PorterStemmer\n",
    "# ps= PorterStemmer()\n",
    "# ps.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transformed_text']=df['text'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc= WordCloud(width=500,height=500,min_font_size=10,background_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=df.rename(columns={'tranformed_text':'transformed_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_wc=wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_wc=wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_corpus=[]\n",
    "for msg in df[df['target']==1]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)\n",
    "len(spam_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_corpus=[]\n",
    "for msg in df[df['target']==0]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        ham_corpus.append(word)\n",
    "len(ham_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts_df = pd.DataFrame(Counter(spam_corpus).most_common(30), columns=['word', 'count'])\n",
    "sns.barplot(x='word', y='count', data=word_counts_df)\n",
    "plt.xticks(rotation=90)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts_df = pd.DataFrame(Counter(ham_corpus).most_common(30), columns=['word', 'count'])\n",
    "sns.barplot(x='word', y='count', data=word_counts_df)\n",
    "plt.xticks(rotation=90)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_building\n",
    "# vectorise>naives bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "tfidf= TfidfVectorizer()\n",
    "cv= CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= cv.fit_transform(df['transformed_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y= df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "#its a function and not a class so it can't be instantiated like other classes such as gaussianNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train,y_test = tts(x,y,test_size=.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "# gnb= GaussianNB()\n",
    "# mnb= MultinomialNB()\n",
    "# bnb= BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnb.fit(x_train,y_train)\n",
    "# y_pred1=gnb.predict(x_test)\n",
    "# print(accuracy_score(y_test,y_pred1))\n",
    "# print(confusion_matrix(y_test,y_pred1))\n",
    "# print(precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb.fit(x_train,y_train)\n",
    "# y_pred2=mnb.predict(x_test)\n",
    "# print(accuracy_score(y_test,y_pred2))\n",
    "# print(confusion_matrix(y_test,y_pred2))\n",
    "# print(precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb.fit(x_train,y_train)\n",
    "# y_pred3=bnb.predict(x_test)\n",
    "# print(accuracy_score(y_test,y_pred3))\n",
    "# print(confusion_matrix(y_test,y_pred3))\n",
    "# print(precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['transformed_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tfidf.fit_transform(df['transformed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not gonna scale coz bad overall\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scalar = MinMaxScaler()\n",
    "# a= scalar.fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train,a_test,b_train,b_test= tts(a,b,test_size=.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnb.fit(a_train,b_train)\n",
    "# b_pred1=gnb.predict(a_test)\n",
    "# print(accuracy_score(b_test,b_pred1))\n",
    "# print(confusion_matrix(b_test,b_pred1))\n",
    "# print(precision_score(b_test,b_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #chosen one\n",
    "# mnb.fit(a_train,b_train)\n",
    "# b_pred2=mnb.predict(a_test)\n",
    "# print(accuracy_score(b_test,b_pred2))\n",
    "# print(confusion_matrix(b_test,b_pred2))\n",
    "# print(precision_score(b_test,b_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb.fit(a_train,b_train)\n",
    "# b_pred3=bnb.predict(a_test)\n",
    "# print(accuracy_score(b_test,b_pred3))\n",
    "# print(confusion_matrix(b_test,b_pred3))\n",
    "# print(precision_score(b_test,b_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we are gonna go with mnb as precision is 100% correct, also didnt take bnb as precision voer accuracy since data is imbalanced\n",
    "# or we can try several other training models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc= SVC(kernel='sigmoid', gamma=1.0)\n",
    "# knc= KNeighborsClassifier()\n",
    "# mnb= MultinomialNB()\n",
    "# dtc= DecisionTreeClassifier(max_depth=5)\n",
    "# lrc= LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc= RandomForestClassifier(n_estimators=50,random_state=2)\n",
    "# abc= AdaBoostClassifier(n_estimators=50,random_state=2)\n",
    "# bc= BaggingClassifier(n_estimators=50,random_state=2)\n",
    "# etc= ExtraTreesClassifier(n_estimators=50,random_state=2)\n",
    "# gbdt= GradientBoostingClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clfs= {'SVC': svc, 'KN': knc, 'NB': mnb, 'DT': dtc,'LR': lrc, 'RF': rfc, 'Adaboost': abc, 'bgc': bc, 'ETC': etc, 'GBDT': gbdt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf,a_train,b_train,a_test,b_test):\n",
    "    clf.fit(a_train,b_train)\n",
    "    b_pred= clf.predict(a_test)\n",
    "    accuracy= accuracy_score(b_test,b_pred)\n",
    "    precision= precision_score(b_test,b_pred) \n",
    "\n",
    "    return accuracy,precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_classifier(svc,a_train,b_train,a_test,b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_scores=[]\n",
    "# precision_scores=[]\n",
    "\n",
    "# for name,clf in clfs.items():\n",
    "#     current_accuracy,current_precision =train_classifier(clf,a_train,b_train,a_test,b_test)\n",
    "#     print('for. ', name)\n",
    "#     print('accuracy- ', current_accuracy)\n",
    "#     print('precision- ', current_precision)\n",
    "\n",
    "#     accuracy_scores.append(current_accuracy)\n",
    "#     precision_scores.append(current_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance_df= pd.DataFrame({'Algorithm': clfs.keys(),'Accuracy': accuracy_scores,'Precision': precision_scores}).sort_values('Precision',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model= rfc.fit(a_train,b_train)\n",
    "metadata = {'scikit-learn_version': sklearn.__version__}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'scikit-learn_version' in metadata:\n",
    "    print(f\"Model trained with scikit-learn version: {metadata['scikit-learn_version']}\")\n",
    "else:\n",
    "    print(\"No scikit-learn version information found in metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_pred=rfc_model.predict(a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranforming_test_data= 'HIIIIIIIII how are you moronic asshole!!!!!  :)))))'\n",
    "test_data= tfidf.transform([transform_text(tranforming_test_data)])\n",
    "rfc_model.predict(test_data)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranforming_test_data2= \"England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/̼1.20 POBOXox36504W45WQ 16+\"\n",
    "test_data2= tfidf.transform([transform_text(tranforming_test_data2)])\n",
    "rfc_model.predict(test_data2)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(b_test,da_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(b_test,da_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to improve model performance\n",
    "# 1. change max features in tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(rfc_model,open('rfc_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump\n",
    "# # dump(tfidf, 'vectorizer.pkl')\n",
    "# dump(rfc_model, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.corpus import stopwords\n",
    "from joblib import load\n",
    "ps= PorterStemmer()\n",
    "# ps.stem\n",
    "\n",
    "st.title('Email/SMS spam classifier')\n",
    "\n",
    "\n",
    "\n",
    "input_sms= st.text_input(\"Enter the message\")\n",
    "transformed_sms= transform_text(input_sms)\n",
    "vector_input= tfidf.transform([transformed_sms])\n",
    "result= rfc_model.predict(vector_input)[0]\n",
    "\n",
    "\n",
    "if result==1:\n",
    "    st.header(\"SPAMMMMMM\")\n",
    "else:\n",
    "    st.header(\"not spam :))))\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'scikit-learn_version' in loaded_metadata:\n",
    "#     print(f\"Model trained with scikit-learn version: {loaded_metadata['scikit-learn_version']}\")\n",
    "# else:\n",
    "#     print(\"No scikit-learn version information found in metadata.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
